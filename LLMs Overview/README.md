
----

## Refrences        
|                                                 Refrences                                                                |
| :----------------------------------------------------------------------------------------------------------------------: |
|          [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                                   | 
|          [GPTQ: Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)    | 
|          [GGML](https://github.com/ggerganov/ggml)                                                                       | 
|          [A Survey on In-context Learning](https://arxiv.org/abs/2301.00234)                                             | 
|          [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)       | 
|          [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)     | 
|          [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)     | 
|          [Automatic Prompt Augmentation and Selection with Chain-of-Thought](https://arxiv.org/abs/2302.12822)           | 
|          [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)                  | 
|          [Large Language Models can Learn Rules](https://arxiv.org/abs/2310.07064)                                       | 
|          [Parameter-Efficient Fine-Tuning without Introducing New Latency](https://arxiv.org/abs/2305.16742)             | 
|          [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2210.03493)                          | 
|          [QLORA:EfficientFinetuningofQuantizedLLMs](https://arxiv.org/pdf/2305.14314.pdf)                                | 
|          [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)       | 
|          [QA-LORA:QUANTIZATION-AWARE LoRA](https://arxiv.org/pdf/2309.14717v2.pdf)                                       | 
|          [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)                    | 
|          [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)                 | 
|          [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496)                  | 
|          [LLM-Eval](https://arxiv.org/abs/2305.13711)                                                                    | 
|          [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)                      | 
|          [NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914)                    | 
|          [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)        | 


----
