import streamlit as st
from PIL import Image
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import time

st.set_page_config(
    page_title="GDSC RAG Demo", layout="wide"
)  # Streamlit sayfa ayarlarÄ±nÄ± yapÄ±landÄ±r

# Logo ekleme (en Ã¼stte ve ortada)
logo = Image.open("gdsc.jpg")  # Logo dosyasÄ±nÄ± aÃ§
col1, col2, col3 = st.columns(
    [1, 1.5, 1]
)  # SÃ¼tunlarÄ± oluÅŸtur, orta sÃ¼tunu daha geniÅŸ yap
with col2:
    st.image(logo, width=1000)  # Logoyu orta sÃ¼tunda gÃ¶rÃ¼ntÃ¼le

st.markdown("""
## GDSC Demo - Belgelerinizden AnÄ±nda Ä°Ã§gÃ¶rÃ¼ler! ğŸš€

Merhaba Google Developer Student Club Ãœyeleri! ğŸ‘‹

Bu demo, Google'Ä±n Ãœretken Yapay Zeka modellerini kullanarak belgelerinizden nasÄ±l anÄ±nda iÃ§gÃ¶rÃ¼ler elde edebileceÄŸinizi gÃ¶steriyor. 
Retrieval-Augmented Generation (RAG) Ã§erÃ§evesi ile gÃ¼Ã§lendirilmiÅŸ bu sohbet robotu, yÃ¼klediÄŸiniz PDF belgelerini analiz ederek sorularÄ±nÄ±za doÄŸru ve hÄ±zlÄ± yanÄ±tlar veriyor.

### NasÄ±l Ã‡alÄ±ÅŸÄ±r? ğŸ¤”

Bu demoyu kullanmak Ã§ok kolay:

1. **API AnahtarÄ±nÄ±zÄ± Girin**: Google AI Studio'dan edindiÄŸiniz API anahtarÄ±nÄ±zÄ± girin. (https://makersuite.google.com/app/apikey)
2. **Belgelerinizi YÃ¼kleyin**: PDF formatÄ±ndaki belgelerinizi yÃ¼kleyin. Birden fazla belge yÃ¼kleyebilirsiniz!
3. **Sorunuzu Sorun**: Belgelerinizle ilgili herhangi bir soruyu sorun ve anÄ±nda yanÄ±t alÄ±n!

**Ä°pucu:** Bu demo, Google'Ä±n Gemini modelini kullanÄ±yor. FarklÄ± modelleri deneyerek sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rabilirsiniz!
""")

# API anahtarÄ± giriÅŸi
api_key = st.text_input(
    "Google API AnahtarÄ±nÄ±zÄ± Girin:", type="password", key="api_key_input"
)  # API anahtarÄ± giriÅŸi iÃ§in bir metin kutusu oluÅŸtur


def get_pdf_text(pdf_docs):
    # PDF belgelerinden metin Ã§Ä±karÄ±r
    text = ""  # Metni saklamak iÃ§in boÅŸ bir dize oluÅŸtur
    for pdf in pdf_docs:  # YÃ¼klenen her PDF belgesi iÃ§in dÃ¶ngÃ¼
        pdf_reader = PdfReader(pdf)  # PDF okuyucuyu oluÅŸtur
        for page in pdf_reader.pages:  # Her sayfa iÃ§in dÃ¶ngÃ¼
            text += page.extract_text()  # Sayfa iÃ§eriÄŸini metne ekle
    return text  # BirleÅŸtirilmiÅŸ metni dÃ¶ndÃ¼r


def get_text_chunks(text):
    # Metni daha kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000, chunk_overlap=1000
    )  # Metin bÃ¶lÃ¼cÃ¼yÃ¼ oluÅŸtur
    chunks = text_splitter.split_text(text)  # Metni parÃ§alara bÃ¶l
    return chunks  # Metin parÃ§alarÄ±nÄ± dÃ¶ndÃ¼r


def get_vector_store(text_chunks, api_key):
    # Metin parÃ§alarÄ±nÄ± vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve FAISS indeksine kaydeder
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", google_api_key=api_key
    )  # GÃ¶mme modelini oluÅŸtur
    vector_store = FAISS.from_texts(
        text_chunks, embedding=embeddings
    )  # VektÃ¶r veritabanÄ±nÄ± oluÅŸtur
    vector_store.save_local("faiss_index")  # VektÃ¶r veritabanÄ±nÄ± kaydet


def get_conversational_chain():
    # Sohbet zincirini oluÅŸturur
    prompt_template = """
    SaÄŸlanan baÄŸlamdan mÃ¼mkÃ¼n olduÄŸunca ayrÄ±ntÄ±lÄ± olarak soruyu yanÄ±tlayÄ±n, tÃ¼m ayrÄ±ntÄ±larÄ± saÄŸladÄ±ÄŸÄ±nÄ±zdan emin olun, eÄŸer cevap
    saÄŸlanan baÄŸlamda deÄŸilse, sadece "cevap baÄŸlamda mevcut deÄŸil" deyin, yanlÄ±ÅŸ cevap vermeyin\n\n
    BaÄŸlam:\n {context}?\n
    Soru: \n{question}\n

    Cevap:
    """  # Prompt ÅŸablonunu tanÄ±mla
    # "gemini-pro" yerine kullanÄ±labilecek baÅŸka bir model deneyin
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro-latest", temperature=0.3, google_api_key=api_key
    )  # Sohbet modelini oluÅŸtur
    prompt = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )  # Prompt ÅŸablonunu oluÅŸtur
    chain = load_qa_chain(
        model, chain_type="stuff", prompt=prompt
    )  # Soru cevaplama zincirini oluÅŸtur
    return chain  # Sohbet zincirini dÃ¶ndÃ¼r


def user_input(user_question, api_key):
    # KullanÄ±cÄ±nÄ±n sorusunu alÄ±r, ilgili belgeleri arar ve yanÄ±t Ã¼retir
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", google_api_key=api_key
    )  # GÃ¶mme modelini oluÅŸtur
    new_db = FAISS.load_local(
        "faiss_index", embeddings, allow_dangerous_deserialization=True
    )  # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
    docs = new_db.similarity_search(user_question)  # Benzer belgeleri ara
    chain = get_conversational_chain()  # Sohbet zincirini oluÅŸtur
    response = chain(
        {"input_documents": docs, "question": user_question}, return_only_outputs=True
    )  # Soruyu sor ve yanÄ±tÄ± al
    st.write("YanÄ±t: ", response["output_text"])  # YanÄ±tÄ± ekrana yazdÄ±r
    time.sleep(15)  # Saniyede 1 istek ile sÄ±nÄ±rlamak iÃ§in 15 saniye bekleyin


def main():
    # st.image(logo, width=100)

    st.header("AI Sohbet Robotu - GDSC Demo ğŸ¤–")

    user_question = st.text_input(
        "PDF DosyalarÄ±nÄ±zdan Bir Soru Sorun:", key="user_question"
    )

    if user_question and api_key:
        user_input(user_question, api_key)

    with st.sidebar:
        st.title("MenÃ¼")
        pdf_docs = st.file_uploader(
            "PDF DosyalarÄ±nÄ±zÄ± YÃ¼kleyin:",
            accept_multiple_files=True,
            key="pdf_uploader",
        )
        if st.button("GÃ¶nder ve Ä°ÅŸle", key="process_button") and api_key:
            with st.spinner("Ä°ÅŸleniyor..."):
                raw_text = get_pdf_text(pdf_docs)
                text_chunks = get_text_chunks(raw_text)
                get_vector_store(text_chunks, api_key)
                st.success("TamamlandÄ±!")


if __name__ == "__main__":
    main()
